{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b70fcc95-5ec8-4d98-98c3-0f71ac63493d",
   "metadata": {},
   "source": [
    "Q1. MinMax Scaler shrinks the data within the given range, usually of 0 to 1. It transforms data by scaling features to a given range. It scales the values to a specific value range without changing the shape of the original distribution.\n",
    "\n",
    "MinMaxScaler is useful when the data has a bounded range or when the distribution is not Gaussian. For example, in image processing, pixel values are typically in the range of 0-255. Scaling these values using MinMaxScaler ensures that the values are within a fixed range and contributes equally to the analysis.\n",
    "\n",
    "It is widely used in two player turn-based games such as Tic-Tac-Toe, Backgammon, Mancala, Chess, etc. In Minimax the two players are called maximizer and minimizer. The maximizer tries to get the highest score possible while the minimizer tries to do the opposite and get the lowest score possible.\n",
    "\n",
    "Example: \n",
    "Consider a game which has 4 final states and paths to reach final state are from root to 4 leaves of a perfect binary tree as shown below. Assume you are the maximizing player and you get the first chance to move, i.e., you are at the root and your opponent at next level. Which move you would make as a maximizing player considering that your opponent also plays optimally?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764dae3b-6cb8-44dc-b4c8-c500ddfe4a62",
   "metadata": {},
   "source": [
    "Q2. The \"Unit Vector\" technique, also known as \"Normalization\" or \"L2 Normalization,\" is a feature scaling method used in machine learning and data preprocessing. It involves scaling each feature in a dataset so that it has a unit norm (length of 1) in the Euclidean space. This technique is particularly useful when the magnitude of the features varies significantly, and you want to ensure that all features contribute equally to the analysis.\n",
    "\n",
    "Mathematically, the unit vector transformation for a feature vector x is calculated as:\n",
    "\n",
    "x_normalized = x / ||x||,\n",
    "\n",
    "where ||x|| represents the Euclidean norm (length) of the vector x.\n",
    "\n",
    "On the other hand, \"Min-Max scaling,\" also known as \"Normalization\" or \"Feature Scaling,\" transforms features by linearly scaling them to a specific range, usually [0, 1], based on their minimum and maximum values.\n",
    "\n",
    "Mathematically, the Min-Max scaling transformation for a feature x is calculated as:\n",
    "\n",
    "x_scaled = (x - min(x)) / (max(x) - min(x)).\n",
    "\n",
    "Difference between Unit Vector Technique and Min-Max Scaling:\n",
    "\n",
    "Normalization Range:\n",
    "\n",
    "Unit Vector: The normalization ensures that the Euclidean norm of the feature vector is 1. All feature values are scaled proportionally to maintain their relative relationships.\n",
    "Min-Max Scaling: The scaling is done to map the feature values within a specific range, typically [0, 1], based on their minimum and maximum values.\n",
    "Normalization Effect:\n",
    "\n",
    "Unit Vector: This technique maintains the direction of the original data points while making them comparable in terms of their magnitudes.\n",
    "Min-Max Scaling: This technique shifts and scales the data points to fit within a specific range, altering their relative distances and relationships.\n",
    "Here's a simple example to illustrate the application of these two techniques:\n",
    "\n",
    "Suppose you have a dataset with two features, \"Height\" (measured in centimeters) and \"Weight\" (measured in kilograms):\n",
    "\n",
    "Sample\tHeight (cm)\tWeight (kg)\n",
    "   \n",
    "    1\t175\t          70\n",
    "   \n",
    "    2   160\t          55\n",
    "    \n",
    "    3\t185           90\n",
    "    \n",
    "    4\t150\t          45\n",
    "    \n",
    "    \n",
    "Unit Vector Technique (Normalization):\n",
    "\n",
    "Calculate the Euclidean norm for each data point:\n",
    "\n",
    "Sample 1: ||(175, 70)|| = √(175^2 + 70^2) ≈ 186.07\n",
    "Sample 2: ||(160, 55)|| = √(160^2 + 55^2) ≈ 168.13\n",
    "Sample 3: ||(185, 90)|| = √(185^2 + 90^2) ≈ 202.68\n",
    "Sample 4: ||(150, 45)|| = √(150^2 + 45^2) ≈ 158.11\n",
    "Normalize each data point:\n",
    "\n",
    "Sample 1: (175/186.07, 70/186.07) ≈ (0.939, 0.347)\n",
    "Sample 2: (160/168.13, 55/168.13) ≈ (0.952, 0.329)\n",
    "Sample 3: (185/202.68, 90/202.68) ≈ (0.912, 0.444)\n",
    "Sample 4: (150/158.11, 45/158.11) ≈ (0.949, 0.284)\n",
    "Min-Max Scaling:\n",
    "\n",
    "Calculate the minimum and maximum values for each feature:\n",
    "\n",
    "Minimum Height: 150, Maximum Height: 185\n",
    "Minimum Weight: 45, Maximum Weight: 90\n",
    "Apply Min-Max Scaling to each data point:\n",
    "\n",
    "Sample 1: ((175 - 150) / (185 - 150), (70 - 45) / (90 - 45)) ≈ (0.625, 0.555)\n",
    "Sample 2: ((160 - 150) / (185 - 150), (55 - 45) / (90 - 45)) ≈ (0.25, 0.333)\n",
    "Sample 3: ((185 - 150) / (185 - 150), (90 - 45) / (90 - 45)) ≈ (1.0, 1.0)\n",
    "Sample 4: ((150 - 150) / (185 - 150), (45 - 45) / (90 - 45)) ≈ (0.0, 0.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6ff1f5-a776-4681-b020-68dbbecc9db9",
   "metadata": {},
   "source": [
    "Q3. Principal Component Analysis (PCA) is a dimensionality reduction technique used in statistics and machine learning to transform high-dimensional data into a lower-dimensional representation while retaining as much of the original variability as possible. PCA achieves this by identifying the principal components, which are orthogonal (uncorrelated) linear combinations of the original features. The first principal component captures the most variance in the data, the second principal component captures the second most, and so on.\n",
    "\n",
    "Here's how PCA works in a nutshell:\n",
    "\n",
    "Standardize the Data: If the features have different scales, it's common practice to standardize the data (subtract mean and divide by standard deviation) to give all features equal importance.\n",
    "\n",
    "Compute the Covariance Matrix: PCA computes the covariance matrix of the standardized data. The covariance matrix describes the relationships between the features and how they vary together.\n",
    "\n",
    "Calculate Eigenvectors and Eigenvalues: The eigenvectors and eigenvalues of the covariance matrix are computed. The eigenvectors represent the directions (principal components) along which the data varies the most, and the eigenvalues represent the amount of variance explained by each principal component.\n",
    "\n",
    "Select Principal Components: The eigenvectors are sorted based on their corresponding eigenvalues in descending order. The top k eigenvectors (principal components) are chosen, where k is the desired reduced dimensionality.\n",
    "\n",
    "Project Data: The original data is projected onto the selected principal components, creating a new lower-dimensional representation of the data.\n",
    "\n",
    "PCA is useful for several purposes, including data visualization, noise reduction, and feature extraction. It's commonly used to reduce the dimensionality of data before applying machine learning algorithms, which can lead to improved efficiency and reduced overfitting.\n",
    "\n",
    "Example of PCA Application:\n",
    "\n",
    "Suppose you have a dataset with two features, \"Height\" and \"Weight,\" measured for different individuals:\n",
    "\n",
    "Sample\tHeight (cm)\tWeight (kg)\n",
    "\n",
    "    1\t175\t         70\n",
    "    \n",
    "    2\t160        \t 55 \n",
    "    \n",
    "    3\t185\t         90\n",
    "    \n",
    "    4\t150\t         45\n",
    "    \n",
    "    \n",
    "Standardize the Data:\n",
    "\n",
    "Subtract mean: (175 + 160) / 2 = 167.5 (mean height), (70 + 55) / 2 = 62.5 (mean weight)\n",
    "Standard deviation: sqrt((175 - 167.5)^2 + (160 - 167.5)^2) / 2 ≈ 8.33 (height), sqrt((70 - 62.5)^2 + (55 - 62.5)^2) / 2 ≈ 7.5 (weight)\n",
    "Standardized data for Sample 1: ((175 - 167.5) / 8.33, (70 - 62.5) / 7.5) ≈ (0.90, 1.00)\n",
    "Standardized data for Sample 2: ((160 - 167.5) / 8.33, (55 - 62.5) / 7.5) ≈ (-0.90, -1.00)\n",
    "\n",
    "Compute Covariance Matrix:\n",
    "\n",
    "Calculate Eigenvectors and Eigenvalues:\n",
    "\n",
    "\n",
    "Select Principal Component:\n",
    "\n",
    "Let's say we decide to keep only the first principal component.\n",
    "\n",
    "Project Data:\n",
    "\n",
    "Project the standardized data onto the first principal component:\n",
    "\n",
    "Projected value for Sample 1: 0.90 * 0.707 + 1.00 * 0.707 ≈ 1.42\n",
    "Projected value for Sample 2: -0.90 * 0.707 + -1.00 * 0.707 ≈ -1.42\n",
    "The original data has been reduced from two dimensions (height and weight) to one dimension (projected value), while still capturing a significant amount of variance in the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf67d72e-e0a7-4612-93aa-66c871edf1d6",
   "metadata": {},
   "source": [
    "Q4. PCA (Principal Component Analysis) is a dimensionality reduction technique that can also be used for feature extraction. In the context of PCA, feature extraction refers to transforming the original high-dimensional feature space into a new, lower-dimensional feature space while retaining the most important information and capturing the underlying structure of the data.\n",
    "\n",
    "The principal components obtained from PCA are often used as new features, which can then be used for various purposes, such as visualization, clustering, classification, or regression. These principal components are orthogonal (uncorrelated) linear combinations of the original features and are ordered by the amount of variance they capture. By selecting a subset of the top principal components, you can effectively reduce the dimensionality of the data while preserving as much relevant information as possible.\n",
    "\n",
    "Example of PCA for Feature Extraction:\n",
    "\n",
    "Suppose you have a dataset with three features, \"Income,\" \"Age,\" and \"Education Level,\" and you want to perform feature extraction using PCA to create new features that capture the most important information. For simplicity, let's consider a small dataset with four samples:\n",
    "\n",
    "Sample\tIncome ($1000)\tAge (years)\tEducation Level\n",
    "\n",
    "    1\t50\t            30\t            14\n",
    "    \n",
    "    \n",
    "    2\t80\t            45\t            16\n",
    "    \n",
    "    3\t60\t            35\t            15\n",
    "   \n",
    "    4\t75\t            40\t            16\n",
    "    \n",
    "Standardize the Data:\n",
    "\n",
    "Compute Covariance Matrix and Eigenvectors/Eigenvalues:\n",
    "\n",
    "Select Principal Components:\n",
    "\n",
    "Project Data:\n",
    "\n",
    "Project the standardized data onto the first two principal components:\n",
    "\n",
    "Eigenvectors: [[0.6, -0.6, 0.5],\n",
    "               [0.7, 0.2, -0.7],\n",
    "               [-0.35, -0.78, -0.52]]\n",
    "\n",
    "Eigenvalues: [0.2, 0.1, 0.05]\n",
    "\n",
    "\n",
    "For Sample 1: (0.6 * (50 - mean_income) / std_income) + (0.7 * (30 - mean_age) / std_age) + (-0.35 * (14 - mean_education) / std_education).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9439589e-5630-42c5-abae-75a76abf9ea4",
   "metadata": {},
   "source": [
    "Q5. Min-Max scaling is a common preprocessing technique used to standardize the range of numerical features in a dataset. In the context of building a recommendation system for a food delivery service, you can use Min-Max scaling to preprocess features like price, rating, and delivery time. Here's how you would apply Min-Max scaling to each feature:\n",
    "\n",
    "Price:\n",
    "Min-Max scaling will ensure that the prices are transformed to a common range, often [0, 1], while preserving the relationships between different price points. This helps prevent features with larger scales from dominating the learning process.\n",
    "\n",
    "The formula for Min-Max scaling is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "068f852f-3a64-4cbe-b463-182204020f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e0447e-4b43-4f34-bb15-d4db47e34dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_price = ($20 - $5) / ($30 - $5) = 0.625"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dcb90b-ea5e-4589-b55f-75767b54eec2",
   "metadata": {},
   "source": [
    "Rating:\n",
    "    \n",
    "Similarly, you can scale the rating feature to the [0, 1] range. If your ratings range from 1 to 5, the scaling would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6bc871-33f5-4648-bb09-e0c941758696",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_rating = (rating - min_rating) / (max_rating - min_rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801fdbaf-3a8a-413c-9055-ad3a26446370",
   "metadata": {},
   "source": [
    "For example, if you have ratings ranging from 2 to 5:\n",
    "\n",
    "Min rating: 2\n",
    "Max rating: 5\n",
    "Applying Min-Max scaling to a rating of 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14be084-c04d-4943-a1d3-ac1d906084cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_rating = (4 - 2) / (5 - 2) = 0.6667"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c652db-1b36-4653-836d-30a6f7a1ef3b",
   "metadata": {},
   "source": [
    "For example, if you have a delivery time of 30 minutes:\n",
    "\n",
    "Min delivery time: 10\n",
    "Max delivery time: 60\n",
    "Applying Min-Max scaling to a delivery time of 30 minutes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321c2220-ef73-40d1-9e1b-a0b7472136de",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_delivery_time = (60 - 30) / (60 - 10)= 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d0c892-61dc-439b-b531-6675da51e28f",
   "metadata": {},
   "source": [
    "Q6. Using PCA for dimensionality reduction in a stock price prediction project can help you reduce the number of features while retaining the most important information for making accurate predictions. Here's how you can apply PCA to the dataset:\n",
    "\n",
    "Data Preprocessing:\n",
    "Start by preparing your dataset. Ensure that all features are appropriately preprocessed, including handling missing values, normalizing or standardizing the data, and encoding categorical variables if necessary.\n",
    "\n",
    "Standardize the Data:\n",
    "Before applying PCA, it's a good practice to standardize the data so that all features have the same scale. This ensures that PCA doesn't give more importance to features with larger values.\n",
    "\n",
    "Compute Covariance Matrix and Eigenvalues/Eigenvectors:\n",
    "Calculate the covariance matrix of the standardized data. Then, compute the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors represent the directions of maximum variance in the data, and the eigenvalues indicate the amount of variance captured by each eigenvector.\n",
    "\n",
    "Sort Eigenvalues and Select Principal Components:\n",
    "Sort the eigenvalues in descending order. The larger the eigenvalue, the more variance the corresponding eigenvector captures. You can then select the top k eigenvectors based on the amount of variance you want to retain. This will determine the reduced dimensionality of the dataset.\n",
    "\n",
    "Project Data onto Principal Components:\n",
    "Project the standardized data onto the selected principal components to create a lower-dimensional representation of the dataset. These projected values will serve as the new features that capture the most important information.\n",
    "\n",
    "Here's a simplified example in Python using the PCA class from the sklearn.decomposition module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26920c43-f448-4252-9b0c-1ac5d614636d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a2aac7d-ccad-4898-a6d3-ff98a169dcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "num_samples = 100\n",
    "num_features = 10\n",
    "data = np.random.rand(num_samples, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d4a3f98a-e888-46cf-9a7f-0a7740bce560",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bfa54a13-b7ce-405a-bd54-b99dd004a0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=5)  # Choose the number of principal components to retain\n",
    "pca_result = pca.fit_transform(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "968fb122-7264-4b1b-97d7-c6599ddefca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance Ratio:\n"
     ]
    }
   ],
   "source": [
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "print(\"Explained Variance Ratio:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "42437904-b91e-4433-a766-8bb3d70d26b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Variance Retained: 0.6467002541046809\n"
     ]
    }
   ],
   "source": [
    "total_variance_retained = sum(explained_variance_ratio)\n",
    "print(\"Total Variance Retained:\", total_variance_retained)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e93df33-f739-4125-9939-bf0b1ece3e4b",
   "metadata": {},
   "source": [
    "Q7. to  perform Min-Max scaling on the dataset [1, 5, 10, 15, 20] transform the values to a range of -1 to 1. Min-Max scaling involves subtracting the minimum value and then dividing by the range (max value - min value).\n",
    "\n",
    "\n",
    "Calculate the minimum and maximum values in the dataset:\n",
    "\n",
    "Minimum value: 1\n",
    "Maximum value: 20\n",
    "\n",
    "Apply Min-Max scaling formula to each value:\n",
    "\n",
    "scaled_value = (original_value - min_value) / (max_value - min_value)\n",
    "To scale to a range of -1 to 1, we'll map the scaled values to the range [0, 2] and then shift them to the range [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "31b04af5-5541-4883-bb08-7e61ec329f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data: [ 1  5 10 15 20]\n",
      "Scaled Data (Range [0, 1]): [0.         0.21052632 0.47368421 0.73684211 1.        ]\n",
      "Scaled Data (Range [-1, 1]): [-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Original dataset\n",
    "original_data = np.array([1, 5, 10, 15, 20])\n",
    "\n",
    "# Calculate min and max values\n",
    "min_value = np.min(original_data)\n",
    "max_value = np.max(original_data)\n",
    "\n",
    "# Apply Min-Max scaling and transform to range -1 to 1\n",
    "scaled_data = (original_data - min_value) / (max_value - min_value)  # Scale to range [0, 1]\n",
    "scaled_data_01 = scaled_data * 2 - 1  # Scale to range [-1, 1]\n",
    "\n",
    "print(\"Original Data:\", original_data)\n",
    "print(\"Scaled Data (Range [0, 1]):\", scaled_data)\n",
    "print(\"Scaled Data (Range [-1, 1]):\", scaled_data_01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a771c06-c646-45b3-b0f8-88698fecddb0",
   "metadata": {},
   "source": [
    "Q8. process of performing feature extraction using PCA on a dataset containing features: [height, weight, age, gender, blood pressure].\n",
    "\n",
    "Sample\tHeight (cm)\tWeight (kg)\tAge (years)\t\tBlood Pressure\n",
    "    \n",
    "    1\t  175\t     70\t        30\t              120\n",
    "    \n",
    "    2\t   160\t     55    \t  45\t             130\n",
    "    \n",
    "    3\t   185\t     90\t       35\t              140\n",
    "    \n",
    "    4\t   150\t     45\t       40\t\t         110\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "37311520-af0f-4568-8df1-998f98ff74e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Principal Components (Eigenvectors):\n",
      " [[ 5.16125237e-01  5.00271449e-01 -4.48149918e-01 -5.31511870e-01]\n",
      " [-3.64604198e-01 -4.92306410e-01 -7.72784495e-01 -1.65838181e-01]\n",
      " [-6.14492958e-01  7.12294094e-01 -2.19361852e-01  2.58681092e-01]\n",
      " [ 4.72310198e-01  6.10622664e-16 -3.92232270e-01  7.89352217e-01]]\n",
      "Explained Variance Ratio: [8.72099809e-01 1.25109044e-01 2.79114660e-03 1.78159728e-33]\n",
      "Cumulative Explained Variance: [0.87209981 0.99720885 1.         1.        ]\n",
      "Number of Principal Components to Retain: 2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Simulated dataset (replace with your actual data)\n",
    "data = np.array([\n",
    "    [175, 70, 30, 0, 120],\n",
    "    [160, 55, 45, 1, 130],\n",
    "    [185, 90, 35, 0, 140],\n",
    "    [150, 45, 40, 1, 110]\n",
    "])\n",
    "\n",
    "# Separate features and target (assuming the last column is the target, i.e., \"Blood Pressure\")\n",
    "X = data[:, :-1]\n",
    "\n",
    "# Step 1: Standardize the data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(X)\n",
    "\n",
    "# Step 2: Apply PCA\n",
    "pca = PCA()\n",
    "pca_result = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Print the principal components\n",
    "print(\"Principal Components (Eigenvectors):\\n\", pca.components_)\n",
    "\n",
    "# Print the explained variance ratio\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "print(\"Explained Variance Ratio:\", explained_variance_ratio)\n",
    "\n",
    "# Print the cumulative explained variance\n",
    "cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
    "print(\"Cumulative Explained Variance:\", cumulative_explained_variance)\n",
    "\n",
    "# Choose the number of principal components to retain\n",
    "num_components_to_retain = np.argmax(cumulative_explained_variance >= 0.95) + 1\n",
    "print(\"Number of Principal Components to Retain:\", num_components_to_retain)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af7256b-765f-4d16-beb5-38ae4c944fba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
